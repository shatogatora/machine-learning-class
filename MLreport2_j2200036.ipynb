{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdb0e46-61d6-47bb-9e78-10d3c265b4b4",
   "metadata": {},
   "source": [
    "## 映画のレビューより感情分析\n",
    "### J2200036 小野慧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e710ff-c557-4dc6-a233-5f179077fdff",
   "metadata": {},
   "source": [
    "#### このレポートでは感情分析を行ってみる。使用するデータは、\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "#### にあるもので、Andrew Maas氏によって提供されたものだ。trainデータセットには映画のレビューが25,000件含まれており、トレーニング用とテスト用にそれぞれ同じ数が提供されている。testデータにも25,000件のレビューが含まれている。また、生のテキストデータと既に処理されたワードバッグ形式のデータがあるが、今回は生のテキストデータを用いる。\n",
    "#### このレポートの目的は、trainデータの映画のレビューコメントから、否定的・肯定的なコメントの特徴を学習してモデルを作り、testデータにて否定か肯定かの二値分類を行うことである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1766e0-0a2c-45fb-b655-5424daebddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0668236-469e-41de-a219-1b5258020378",
   "metadata": {},
   "source": [
    "#### 直接データセットをローカルにダウンロードし、展開するにはかなり時間がかかったので、tf.keras.utils.get_file()という関数を用いて、ページから直接ダウンロードする。この関数は、Kerasのユーティリティ関数の一部であり、TensorFlowの中にあるKerasモジュールにて利用できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eecfb11-12ae-49a7-9930-06bb01d06e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのダウンロード\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7af2ddc-5c3d-482d-9253-4e6f6645a9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbEr.txt', 'imdb.vocab', '.ipynb_checkpoints', 'test', 'train', 'README']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_dirの下部構造を確認\n",
    "\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9599e0-10df-43b3-b1b8-8946a7d81f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'unsupBow.feat',\n",
       " 'urls_pos.txt',\n",
       " 'pos',\n",
       " 'urls_neg.txt',\n",
       " 'urls_unsup.txt',\n",
       " 'unsup']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dirの下部構造を確認\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612407c8-8291-4ea1-9343-47c86bde10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "# 例として、肯定的なレビューをひとつ確認\n",
    "\n",
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bf6f8-ae9d-4056-be9a-d50d22e94b16",
   "metadata": {},
   "source": [
    "#### 学習用のデータセットを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b079ec8-1cec-46ed-9031-b77f4e277eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 余分なフォルダを削除\n",
    "\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8abf8e-8a3b-480a-b5f1-8529aec843bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "# trainデータを8:2に分割、うち8割をtrainデータとする\n",
    "\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e23ee5-e4a4-4802-a6cb-9efa4ebb2f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# trainデータを8:2に分割、うち2割を検証データとする\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52080ae1-36c4-457d-9c2e-47feef9803db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# testデータをバッチサイズ32に分割\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bc21e55-dcf4-4903-9627-a3a123e983cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストの前処理\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a353a049-e7b1-4013-abd1-e74fd03551bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストをベクトル化するレイヤーを定義\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae5399c-7b8b-4a69-86a3-6052acb08ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル化レイヤーをトレーニングデータに合わせて調整\n",
    "\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80f2d09d-71df-4645-9a56-e8738cbc1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータを数値化する関数を定義\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad6f04cf-dcdc-4ab6-9498-2770481812f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'Recipe for one of the worst movies of all time: a she-male villain who looks like it escaped from the WWF, has terrible aim with a gun that has inconsistent effects (the first guy she shoots catches on fire but when she shoots anyone else they just disappear) and takes time out to pet a deer. Then you got the unlikable characters, 30 year old college students, a lame attempt at a surprise ending and lots, lots more. Avoid at all costs.', shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[9257,   15,   28,    5,    2,  241,   91,    5,   30,   58,    4,\n",
      "           1, 1011,   36,  262,   38,    9, 3891,   35,    2,    1,   43,\n",
      "         382, 5223,   16,    4, 1113,   12,   43, 5739,  300,    2,   83,\n",
      "         225,   55, 3209, 3898,   20,  973,   18,   51,   55, 3209,  250,\n",
      "         320,   34,   40, 4386,    3,  294,   58,   44,    6, 2911,    4,\n",
      "        6757,   92,   22,  184,    2, 4916,  100, 1221,  336,  161, 1199,\n",
      "        1484,    4,  808,  568,   31,    4,  839,  270,    3,  741,  741,\n",
      "          50,  774,   31,   30, 2070,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# テキストデータを数値化\n",
    "\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4218757-501a-4010-b636-5ce78d05ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レイヤーを適用\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5ff18f3-dc7f-431b-b7b4-0dcfaadd838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af28e6fa-ee2b-407e-81f0-f631b7a72bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160033 (625.13 KB)\n",
      "Trainable params: 160033 (625.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデルの作成\n",
    "\n",
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features + 1, embedding_dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b0e7e88-603e-4786-8ee3-77189a27f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数、オプティマイザ、評価メトリクスの設定\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "380a0894-c2ba-405d-b917-5a5432d86e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 7s 10ms/step - loss: 0.6625 - binary_accuracy: 0.6989 - val_loss: 0.6136 - val_binary_accuracy: 0.7720\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.5477 - binary_accuracy: 0.8023 - val_loss: 0.4982 - val_binary_accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.4446 - binary_accuracy: 0.8439 - val_loss: 0.4202 - val_binary_accuracy: 0.8484\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3783 - binary_accuracy: 0.8668 - val_loss: 0.3737 - val_binary_accuracy: 0.8614\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3353 - binary_accuracy: 0.8802 - val_loss: 0.3448 - val_binary_accuracy: 0.8678\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3041 - binary_accuracy: 0.8895 - val_loss: 0.3257 - val_binary_accuracy: 0.8726\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2807 - binary_accuracy: 0.8978 - val_loss: 0.3127 - val_binary_accuracy: 0.8736\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2615 - binary_accuracy: 0.9052 - val_loss: 0.3030 - val_binary_accuracy: 0.8756\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2450 - binary_accuracy: 0.9102 - val_loss: 0.2961 - val_binary_accuracy: 0.8778\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2315 - binary_accuracy: 0.9169 - val_loss: 0.2922 - val_binary_accuracy: 0.8784\n"
     ]
    }
   ],
   "source": [
    "# モデルのトレーニング\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "948c7af9-bab6-46ae-9a1f-98e86092233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3111 - binary_accuracy: 0.8718\n",
      "Loss:  0.31110289692878723\n",
      "Accuracy:  0.8717600107192993\n"
     ]
    }
   ],
   "source": [
    "# モデルの評価\n",
    "\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f86ae2-de37-401e-9729-a85b7040a374",
   "metadata": {},
   "source": [
    "#### よって上記の通りに、約87.1%の精度でテキストデータの二値分類予測ができた。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6007c-c9b5-47f2-ba73-c4f5b708e88e",
   "metadata": {},
   "source": [
    "#### 引用\n",
    "Stanford AI Group Website: https://ai.stanford.edu/~amaas/data/sentiment/\r\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher\n",
    "    title     = {Learning Word Vectors for Sentiment Analysi}  \n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologi\n",
    "  \r\n",
    "  month     = {J\n",
    "  ,\r\n",
    "  year      = {\n",
    "  },\r\n",
    "  address   = {Portland, Oregon\n",
    "  A},\r\n",
    "  publisher = {Association for Computational Lingu\n",
    "  cs},\r\n",
    "  pages     = {1\n",
    "  150},\r\n",
    "  url       = {http://www.aclwebt#-1015}\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf47ff0-e8bf-482f-b720-331a9991db60",
   "metadata": {},
   "source": [
    "#### 参考\n",
    "Tensorflow: https://www.tensorflow.org/tutorials/keras/text_classification?hl=ja-1015}\n",
    "\n",
    "note: https://note.com/hroy/n/ncb1469281410"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
